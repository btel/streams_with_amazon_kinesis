{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* intro to kinesis\n",
    "* configure your AWS credentials\n",
    "* KCL wordputter\n",
    "* implementing simple processor class (echo)\n",
    "* implementing simple counter (no ordering)\n",
    "* testing with two KCL wordputters with lag (simulating long network delay)\n",
    "* implement counter with buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Sqreen we use AWS Kinesis service to process data from our agents in near real-time. This kind of processing became recently popular with the appearance of general use libraries that support it (such as [Apache Kafka](https://kafka.apache.org/)). Since such libraries deal with the stream of data, the name of such processing became \"stream processing\". It's a departure from the old model of analytics that ran analysis in batches (hence its name \"batch processing\"). The main differences between these two approaches are:\n",
    "\n",
    "* stream processing deals with data that are punctual in time, i.e. with events that are generated at specific points in time, whereas batch processing is applied to data batches (for example, stored in databases),\n",
    "* stream processing analyses data online, i.e. almost immediately after it arrives, whereas batch processing waits for the data collection to be finished (the moment can be defined arbitarily, for example, at the end of the day) to analyse it off-line,\n",
    "* data analysed by stream processing is unbounded, i.e. it does not have the specific end, whereas the batches are bounded, i.e. they have a well defined size.\n",
    "\n",
    "Libraries such as [Apache Kafka](https://kafka.apache.org/) provide streams that receive data from the event  sources (producers) and pass them down to the consumers, which in turn can forward them to other streams. In essence they are similar to message queues, but they support multiple consumers that process the same messages in parallel (like in [publish-subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) messaging model) and store the old messages even after they were delivered to the consumers. They are a kind of append-only event logs. Although, logs are most commonly associated with the append only flat files siting in the `\\var\\log` directory and containing information logged by your operating system, streams are logs that are optimised for storing/provisioning  binary data (that could be text but also fragments of images, sensor readings, etc.). This log-like design of streams allows new consumers to be added and read from the stream starting with any offset (any message in the past) or removed without any impact on the remaining consumers. Since a single machine may not be able to process high-frequency events created in real web applications, both streams and their consumers can be distributed by partitioning the source events.\n",
    "\n",
    "Streams have found applications in many problems. They are commonly used for real-time data analytics (such as streams of twits), for replicating databases (both for performance and reliability reasons), for real-time monitoring and detection of special events (such as fraud detection) and for building data-intensive systems that require different representations of the same data (for example, databases for operations, indexes for fast queries, and data warehouses for running batch analyses).\n",
    "\n",
    "[Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/introduction.html) is a managed service that provides streaming platform. It includes stream storage and an API to implement producers and consumers. Amazon charges per hour of each stream partition (called shards in Kinesis) and per volume of data flowing through the stream.\n",
    "\n",
    "* what is stream and stream processing\n",
    "* difference between streams and queues\n",
    "* applications of stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to familirize you with the stream processing with Amazon Kinesis. In particular, we will try to implement a simple producer-stream-consumer pipeline that counts the number of requests in a consecutive, one-minute-long time windows. We will apply this pipeline to simulated data, but it could be easily extended to work with real websites. This is precisely one of the applications that we use Kinesis for at Sqreen (more about it below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install dependencies, run the following commands at the command line (i.e. in the shell).\n",
    "    \n",
    "```\n",
    "pip install aws\n",
    "pip install boto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to AWS, you must first create your credentials (you will get them from the AWS Console). Then, simply configure them using the following command:\n",
    "\n",
    "```aws configure --profile blogpost-kinesis```\n",
    "\n",
    "`blogpost-kinesis` is the name of the profile you will use for this tutorial. When requested you will need to copy-paste you public and secret access keys obtained from [AWS Management Console](https://console.aws.amazon.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first stream. You can either do it using the AWS Console or the API. We will use the second approach. First, we need to define the name of the stream, the region in which we will create it, and the profile to use for our AWS credentials (you can set it to `None` if you use the default profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name =  'blogpost-word-stream'\n",
    "region = 'eu-west-1'\n",
    "aws_profile = 'blogpost-kinesis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `boto` library to create the stream and wait until it becomes active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream blogpost-word-stream created in region eu-west-1\n",
      "stream blogpost-word-stream is active\n"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "from boto.kinesis.exceptions import ResourceInUseException\n",
    "import os\n",
    "import time\n",
    "\n",
    "if aws_profile:\n",
    "    os.environ['AWS_PROFILE'] = aws_profile\n",
    "\n",
    "# connect to the kinesis\n",
    "kinesis = boto.kinesis.connect_to_region(region)\n",
    "\n",
    "try:\n",
    "    # create the stream\n",
    "    kinesis.create_stream(stream_name, 1)\n",
    "    print('stream {} created in region {}'.format(stream_name, region))\n",
    "except ResourceInUseException:\n",
    "    print('stream {} already exists in region {}'.format(stream_name, region))\n",
    "\n",
    "def get_status():\n",
    "    r = kinesis.describe_stream(stream_name)\n",
    "    description = r.get('StreamDescription')\n",
    "    status = description.get('StreamStatus')\n",
    "    return status\n",
    "\n",
    "# wait for the stream to become active\n",
    "while get_status() != 'ACTIVE':\n",
    "    time.sleep(1)\n",
    "print('stream {} is active'.format(stream_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting data into streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have operational stream processing, we need a source of the messagers (producer in AWS terminology) and receiver (consumer) that will obtain and process the messages. We will first define the producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import threading\n",
    "from boto.kinesis.exceptions import ResourceNotFoundException\n",
    "\n",
    "class KinesisProducer(threading.Thread):\n",
    "    \"\"\"Producer class for AWS Kinesis streams\n",
    "    \n",
    "    This class will emit records with the IP addresses as partition key and\n",
    "    the emission timestamps as data\"\"\"\n",
    "    \n",
    "    def __init__(self, stream_name, sleep_interval=None, ip_addr='8.8.8.8'):\n",
    "        self.stream_name = stream_name\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.ip_addr = ip_addr\n",
    "        super().__init__()\n",
    "        \n",
    "    def put_record(self):\n",
    "        \"\"\"put a single record to the stream\"\"\"\n",
    "        timestamp = datetime.datetime.utcnow()\n",
    "        part_key = self.ip_addr\n",
    "        data = timestamp.isoformat()\n",
    "\n",
    "        kinesis.put_record(self.stream_name, data, part_key)\n",
    "    \n",
    "    def run_continously(self):\n",
    "        \"\"\"put a record at regular intervals\"\"\"\n",
    "        while True:\n",
    "            self.put_record()\n",
    "            time.sleep(self.sleep_interval)\n",
    "                \n",
    "    def run(self):\n",
    "        try:\n",
    "            if self.sleep_interval:\n",
    "                self.run_continously()\n",
    "            else:\n",
    "                self.put_record()\n",
    "        except ResourceNotFoundException:\n",
    "            print('stream {} not found. Exiting'.format(self.stream_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the partion key we used the IP address and for the data the timestamps. In theory, you are almost completely free to choose whatever you want for the data, as long as it can be serialised in binary format and it's less than 50 KB of size. If you need emit larger data, you need to split it into several records. The partion key must be a string shorter than 256 characters, it will be used to determine which shard to send the data to (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we implemented the `KinesisProducer` as a Python thread, such that it can run in the background and won't block the Python interpreter. This way we can continue executing Python instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create two of such producers with different IP addresses and different intervals between consecutive messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer1 = KinesisProducer(stream_name, sleep_interval=2, ip_addr='8.8.8.8')\n",
    "producer2 = KinesisProducer(stream_name, sleep_interval=5, ip_addr='8.8.8.9')\n",
    "producer1.start()\n",
    "producer2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqreen's Security Automation feature allows one to monitor traffic at the website and set conditions under which a given client should be blocked (such as, trying to read the same page too many times). To implement this feature, we are running similar event sources that inform the stream about the IP addresses from which the requests are emitted together with the timestamp of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming from stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumers receive the messages from the stream and process them. Their output could be messages forwared to another stream, file saved on the filesystem (or Amazon S3 storage) or records stored in a database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a generic consumer, which will consist of `run` method polling for new events from the Kinesis stream and `process_method` that will process the event data and produce any of the side effects (i.e. forwarding the results to another stream or commiting them to a database). The `process_method` will not be implemented in this generic base class, and it will need to be implemented in the sub-classes (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.kinesis.exceptions import ProvisionedThroughputExceededException\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws-samples/kinesis-poster-worker/blob/master/worker.py\n",
    "\n",
    "class KinesisConsumer:\n",
    "    \"\"\"Generic Consumer for Amazon Kinesis Streams\"\"\"\n",
    "    def __init__(self, stream_name, shard_id, iterator_type,\n",
    "                 worker_time=30, sleep_interval=0.5):\n",
    "   \n",
    "        self.stream_name = stream_name\n",
    "        self.shard_id = str(shard_id)\n",
    "        self.iterator_type = iterator_type\n",
    "        self.worker_time = worker_time\n",
    "        self.sleep_interval = sleep_interval\n",
    "        \n",
    "    def process_records(self, records):\n",
    "        \"\"\"the main logic of the Consumer that needs to be implemented\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def iter_records(records):\n",
    "        for record in records:\n",
    "            part_key = record['PartitionKey']\n",
    "            data = record['Data']\n",
    "            yield part_key, data\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"poll stream for new records and forward them to process_records method\"\"\"\n",
    "        response = kinesis.get_shard_iterator(self.stream_name,\n",
    "            self.shard_id, self.iterator_type)\n",
    "        \n",
    "        next_iterator = response['ShardIterator']\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "        finish = start + datetime.timedelta(seconds=self.worker_time)\n",
    "        \n",
    "        while finish > datetime.datetime.now():\n",
    "            try:\n",
    "                response = kinesis.get_records(next_iterator, limit=25)\n",
    "        \n",
    "                records = response['Records']\n",
    "            \n",
    "                if records:\n",
    "                    self.process_records(records)\n",
    "            \n",
    "                next_iterator = response['NextShardIterator']\n",
    "                time.sleep(self.sleep_interval)\n",
    "            except ProvisionedThroughputExceededException as ptee:\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each stream can have many consumers that receive all the messages and process them independently. Now, we will implement `process_records` method that will simply print the received messages to the `stdout`. We will do that by sub-classing the `KinesisConsumer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoConsumer(KinesisConsumer):\n",
    "    \"\"\"Consumers that echos received data to standard output\"\"\"\n",
    "    def process_records(self, records):\n",
    "        \"\"\"print the partion key and data of each incoming record\"\"\"\n",
    "        for part_key, data in self.iter_records(records):\n",
    "            print(part_key, \":\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attach the consumer to our stream. To do that we need to pass the shard ID and the position in the stream to start processing the messages. For the later, we can choose between that newest (`LATEST`) or the oldest (`TRIM_HORIZON`) record in the stream. Note that the default retention period for messages in Kinesis streams is 24 hours. It can be extended up to 168 hours at an additional cost.\n",
    "\n",
    "The streams are partitioned into seperate \"sub-streams\" (called shards) that receive messages from the same source. The target shard for each message is determined from the partion key. Each consumer can read from one or more shards, but there must be at least one consumer per shard, otherwise some messages will be lost. Since, we only use one shard in this example, we can directly pass the default shard ID. If you need to configure more than one shard (to increase the throughput), you will need to query the stream for the IDs of all active shards using the API. For the sake of this tutorial, we will assume that we have only a single shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_id = 'shardId-000000000000'\n",
    "iterator_type =  'LATEST'\n",
    "worker = EchoConsumer(stream_name, shard_id, iterator_type, worker_time=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.8.8.8 : 2018-07-31T21:56:42.972560\n",
      "8.8.8.8 : 2018-07-31T21:56:45.093759\n",
      "8.8.8.9 : 2018-07-31T21:56:46.252859\n",
      "8.8.8.8 : 2018-07-31T21:56:47.200131\n",
      "8.8.8.8 : 2018-07-31T21:56:49.331134\n"
     ]
    }
   ],
   "source": [
    "worker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the consumer printed all received records with their partion keys (IP addresses) and data (timestamps). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can implement a consumer with some non-trivial logic. The goal of this consumer is to count the number of distinct requests from each particular IP in a specific time window (here one minute). Again, we will subclass the `KinesisConsumer` class and re-implement the `process_records` method. In addition, we will define one extra helper method `print_counters` that will simply dump the current counts to the standard output. In practice, we would forward the outputs of such processing to another stream for further analysis (filtering, detection of untypical events etc.) or store it in the DB. This is what actually happens in Sqreen's Security Automation pipeline.\n",
    "\n",
    "[TODO: Should add here a simplified structure of Automation pipeline?]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dateutil import parser\n",
    "from operator import itemgetter\n",
    "\n",
    "class CounterConsumer(KinesisConsumer):\n",
    "    \"\"\"Consumer that counts IP occurences in 1-minute time buckets\"\"\"\n",
    "    \n",
    "    def __init__(self, stream_name, shard_id, iterator_type, worker_time):\n",
    "        self.time_buckets = defaultdict(Counter)\n",
    "        sleep_interval = 20 # seconds\n",
    "        super().__init__(stream_name, shard_id, iterator_type, worker_time, sleep_interval)\n",
    "        \n",
    "    def print_counters(self):\n",
    "        \"\"\"helper method to show counting results\"\"\"\n",
    "        \n",
    "        now = datetime.datetime.utcnow()\n",
    "        print(\"##### Last run at {}\".format(now))\n",
    "        for timestamp, ip_counts in self.time_buckets.items():\n",
    "            # sort counts with respect to the IP address\n",
    "            ip_counts = sorted(ip_counts.items(), key=itemgetter(0))\n",
    "            print(timestamp, ':', list(ip_counts))\n",
    "            \n",
    "    def process_records(self, records):\n",
    "        for ip_addr, timestamp_str in self.iter_records(records):\n",
    "            timestamp = parser.parse(timestamp_str)\n",
    "            timestamp = timestamp.replace(second=0, microsecond=0)\n",
    "            self.time_buckets[timestamp][ip_addr] += 1\n",
    "        self.print_counters()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the consumer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Last run at 2018-07-31 21:57:12.333587\n",
      "2018-07-31 21:56:00 : [('8.8.8.8', 4), ('8.8.8.9', 1)]\n",
      "2018-07-31 21:57:00 : [('8.8.8.8', 5), ('8.8.8.9', 3)]\n",
      "##### Last run at 2018-07-31 21:57:32.456236\n",
      "2018-07-31 21:56:00 : [('8.8.8.8', 4), ('8.8.8.9', 1)]\n",
      "2018-07-31 21:57:00 : [('8.8.8.8', 15), ('8.8.8.9', 7)]\n",
      "##### Last run at 2018-07-31 21:57:52.605160\n",
      "2018-07-31 21:56:00 : [('8.8.8.8', 4), ('8.8.8.9', 1)]\n",
      "2018-07-31 21:57:00 : [('8.8.8.8', 24), ('8.8.8.9', 10)]\n",
      "##### Last run at 2018-07-31 21:58:12.749545\n",
      "2018-07-31 21:56:00 : [('8.8.8.8', 4), ('8.8.8.9', 1)]\n",
      "2018-07-31 21:57:00 : [('8.8.8.8', 28), ('8.8.8.9', 12)]\n",
      "2018-07-31 21:58:00 : [('8.8.8.8', 6), ('8.8.8.9', 2)]\n",
      "##### Last run at 2018-07-31 21:58:32.902239\n",
      "2018-07-31 21:56:00 : [('8.8.8.8', 4), ('8.8.8.9', 1)]\n",
      "2018-07-31 21:57:00 : [('8.8.8.8', 28), ('8.8.8.9', 12)]\n",
      "2018-07-31 21:58:00 : [('8.8.8.8', 15), ('8.8.8.9', 6)]\n"
     ]
    }
   ],
   "source": [
    "worker = CounterConsumer(stream_name, shard_id, iterator_type, worker_time=120)\n",
    "worker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the lines seperated by the hash signs `#####` show the results of the counting process for a single run of the consumer. Since the consumer is executed each time new events arrive, the lines show updated state of the `time_buckets` cache. Each line starts with the timestamp denoting the beginning of the time bucket (it ends with the beginning of the next time bucket, i.e. the windows do not overlap), and the it's followed by the list of IP address and count pairs. Every time the consumer runs the values are updated, such that the counts increase. If new requests arrive at the time that is not covered by any of the buckets, a new bucket is added and the count starts from zero for this bucket. The effect is roughly what we tried to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrated how to use Amazon Kinesis on a simple request counting example. Although the example was simplified, it contained the basic components of all stream processors &mdash; the two producers, a stream (with a signle partition) and one consumer. You can easily take this example and adapt it to your needs. \n",
    "\n",
    "One important limitation of the present `CounterConsumer` is that it keeps in memory and print all windows at each run of the consumer. In real applications we might want to save only the completed windows in the database and remove them from the `time_buckets` cache, but this would require to know when a window is closed and no new events will arrive that would fit in this window. This may not be a trivial problem, because we can never be sure whether some events will arrive late, for example due to some network delay or temporary network outage.\n",
    "\n",
    "Another extension of `CounterConsumer` is to allow for an overlap between the windows. This overlap would provide some smoothing in the counts and make our pipeline more responsive, because the end user would not have to wait for the full window to be complete before seeing a new event being added to the counts.\n",
    "\n",
    "Last, but not least we did not cover important topic of spawning new consumers in case when the existing consumer fails or we want to scale up the application and checkpointing the state of the consumer. This process can be automatised using [Amazon Kinesis Client library](https://github.com/awslabs/amazon-kinesis-client-python), but with the complication that you would need to start the consumer by means of Java-based orchestrator (called MultiLangDaeamon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream blogpost-word-stream not found. Exiting\n",
      "stream blogpost-word-stream not found. Exiting\n"
     ]
    }
   ],
   "source": [
    "# delete the stream at the end of the exercise to minimize AWS costs\n",
    "kinesis.delete_stream(stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sqreen-backend)",
   "language": "python",
   "name": "sqreen-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
