{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* intro to kinesis\n",
    "* configure your AWS credentials\n",
    "* KCL wordputter\n",
    "* implementing simple processor class (echo)\n",
    "* implementing simple counter (no ordering)\n",
    "* testing with two KCL wordputters with lag (simulating long network delay)\n",
    "* implement counter with buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Sqreen we are using AWS Kinesis service to process data from our agents in near real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install dependencies, run the following commands at the command line (i.e. in the shell).\n",
    "    \n",
    "```\n",
    "pip install aws\n",
    "pip install boto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to AWS, you must first create your credentials (you will get them from the AWS Console). Then, simply configure them using the following command:\n",
    "\n",
    "```aws configure --profile blogpost-kinesis```\n",
    "\n",
    "`blogpost-kinesis` is the name of the profile you will use for this tutorial. You will need to copy you public and secret access keys obtained from AWS Management Console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first stream. You can either do it using the AWS Console or the API. We will use the second approach. First, we need to define the name of the stream, the region in which we will create it, and the profile to use for our AWS credentials (you can set it to `None` if you use the default profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name =  'blogpost-word-stream'\n",
    "region = 'eu-west-1'\n",
    "aws_profile = 'blogpost-kinesis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `boto` library to create the stream and wait until it becomes active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream blogpost-word-stream already exists in region eu-west-1\n",
      "stream blogpost-word-stream is active\n"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "from boto.kinesis.exceptions import ResourceInUseException\n",
    "import os\n",
    "import time\n",
    "\n",
    "if aws_profile:\n",
    "    os.environ['AWS_PROFILE'] = aws_profile\n",
    "\n",
    "# connect to the kinesis\n",
    "kinesis = boto.kinesis.connect_to_region(region)\n",
    "\n",
    "try:\n",
    "    # create the stream\n",
    "    kinesis.create_stream(stream_name, 1)\n",
    "    print('stream {} created in region {}'.format(stream_name, region))\n",
    "except ResourceInUseException:\n",
    "    print('stream {} already exists in region {}'.format(stream_name, region))\n",
    "\n",
    "def get_status():\n",
    "    r = kinesis.describe_stream(stream_name)\n",
    "    description = r.get('StreamDescription')\n",
    "    status = description.get('StreamStatus')\n",
    "    return status\n",
    "\n",
    "# wait for the stream to become active\n",
    "while get_status() != 'ACTIVE':\n",
    "    time.sleep(1)\n",
    "print('stream {} is active'.format(stream_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting data into streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have operational stream processing, we need a source of the messagers (producer in AWS terminology) and receiver (consumer) that will obtain and process the messages. We will first define the producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import threading\n",
    "from boto.kinesis.exceptions import ResourceNotFoundException\n",
    "\n",
    "class KinesisProducer(threading.Thread):\n",
    "    \"\"\"Producer class for AWS Kinesis streams\n",
    "    \n",
    "    This class will emit records with the IP addresses as partition key and\n",
    "    the emission timestamps as data\"\"\"\n",
    "    \n",
    "    def __init__(self, stream_name, sleep_interval=None, ip_addr='8.8.8.8'):\n",
    "        self.stream_name = stream_name\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.ip_addr = ip_addr\n",
    "        super().__init__()\n",
    "        \n",
    "    def put_record(self):\n",
    "        \"\"\"put a single record to the stream\"\"\"\n",
    "        timestamp = datetime.datetime.utcnow()\n",
    "        part_key = self.ip_addr\n",
    "        data = timestamp.isoformat()\n",
    "\n",
    "        kinesis.put_record(self.stream_name, data, part_key)\n",
    "    \n",
    "    def run_continously(self):\n",
    "        \"\"\"put a record at regular intervals\"\"\"\n",
    "        while True:\n",
    "            self.put_record()\n",
    "            time.sleep(self.sleep_interval)\n",
    "                \n",
    "    def run(self):\n",
    "        try:\n",
    "            if self.sleep_interval:\n",
    "                self.run_continously()\n",
    "            else:\n",
    "                self.put_record()\n",
    "        except ResourceNotFoundException:\n",
    "            print('stream {} not found. Exiting'.format(self.stream_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the partion key we used the IP address and for the data the timestamps. In theory, you are almost completely free to choose whatever you want for the data, as long as it's less than 50 KB of size. If you need emit larger data, you need to split it into several records. The partion key must be a string shorter than 256 characters, it will be used to determine which shard to send the data to (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we implemented the KinesisProducer as a python thread, such that it can run in the background and won't block Python REPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create two of such producers with different IP addresses and different intervals between emitting the consecutive messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer1 = KinesisProducer(stream_name, sleep_interval=2, ip_addr='8.8.8.8')\n",
    "producer2 = KinesisProducer(stream_name, sleep_interval=5, ip_addr='8.8.8.9')\n",
    "producer1.start()\n",
    "producer2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming from stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumers receive the messages from the stream and process them. Their output could be messages forwared to another stream, file saved in the filesystem (or Amazon S3 storage) or records stored in a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.kinesis.exceptions import ProvisionedThroughputExceededException\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws-samples/kinesis-poster-worker/blob/master/worker.py\n",
    "\n",
    "class KinesisWorker:\n",
    "    \"\"\"Generic Consumer for Amazon Kinesis Streams\"\"\"\n",
    "    def __init__(self, stream_name, shard_id, iterator_type,\n",
    "                 worker_time=30, sleep_interval=0.5):\n",
    "   \n",
    "        self.stream_name = stream_name\n",
    "        self.shard_id = str(shard_id)\n",
    "        self.iterator_type = iterator_type\n",
    "        self.worker_time = worker_time\n",
    "        self.sleep_interval = sleep_interval\n",
    "        \n",
    "    def process_records(self, records):\n",
    "        \"\"\"the main logic of the Consumer that needs to be implemented\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def iter_records(records):\n",
    "        for record in records:\n",
    "            part_key = record['PartitionKey']\n",
    "            data = record['Data']\n",
    "            yield part_key, data\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"poll stream for new records and forward them to process_records method\"\"\"\n",
    "        response = kinesis.get_shard_iterator(self.stream_name,\n",
    "            self.shard_id, self.iterator_type)\n",
    "        \n",
    "        next_iterator = response['ShardIterator']\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "        finish = start + datetime.timedelta(seconds=self.worker_time)\n",
    "        \n",
    "        while finish > datetime.datetime.now():\n",
    "            try:\n",
    "                response = kinesis.get_records(next_iterator, limit=25)\n",
    "        \n",
    "\n",
    "                records = response['Records']\n",
    "            \n",
    "                if records:\n",
    "                    self.process_records(records)\n",
    "            \n",
    "                next_iterator = response['NextShardIterator']\n",
    "                time.sleep(self.sleep_interval)\n",
    "            except ProvisionedThroughputExceededException as ptee:\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each stream can have many consumers that receive all the messages and process them independently. Now, we will implement `process_records` method that will simply print the received messages to the `stdout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoWorker(KinesisWorker):\n",
    "    \"\"\"Consumers that echos received data to standard output\"\"\"\n",
    "    def process_records(self, records):\n",
    "        \"\"\"print the partion key and data of each incoming record\"\"\"\n",
    "        for part_key, data in self.iter_records(records):\n",
    "            print(part_key, \":\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the consumer on our stream. Note that we need to pass the shard ID and the position in the stream to start processing the messages. For the later, we can choose between that newest (`LATEST`) or the oldest (`TRIM_HORIZON`) record in the stream. \n",
    "\n",
    "The streams are partitioned into seperate \"sub-streams\" (called shards) that receive messages from the same source. The target shard for each message is determined from the partion key. Each consumer can read from one or more shards, but there must be at least one consumer per shard, otherwie some messages will be lost. Since, we only use one shard in this example, we can directly pass the default shard ID. If you need to configure more than one shard (to increase the throughput), you will need to query the stream for the IDs of all active shards using the API. For the sake of this tutorial, we will assume that we have only a single shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_id = 'shardId-000000000000'\n",
    "iterator_type =  'LATEST'\n",
    "worker = EchoWorker(stream_name, shard_id, iterator_type, worker_time=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.8.8.8 : 2018-07-29T22:35:46.694763\n",
      "8.8.8.8 : 2018-07-29T22:35:48.855009\n",
      "8.8.8.9 : 2018-07-29T22:35:49.951666\n",
      "8.8.8.8 : 2018-07-29T22:35:51.015702\n",
      "8.8.8.8 : 2018-07-29T22:35:53.134880\n"
     ]
    }
   ],
   "source": [
    "worker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the consumer printed all received records with their partion keys (IP addresses) and data (timestamps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dateutil import parser\n",
    "from operator import itemgetter\n",
    "\n",
    "class CounterWorker(KinesisWorker):\n",
    "    \"\"\"Consumer that counts IP occurences in 1-minute time buckets\"\"\"\n",
    "    \n",
    "    def __init__(self, stream_name, shard_id, iterator_type, worker_time):\n",
    "        self.time_buckets = defaultdict(Counter)\n",
    "        sleep_interval = 20 # seconds\n",
    "        super().__init__(stream_name, shard_id, iterator_type, worker_time, sleep_interval)\n",
    "        \n",
    "    def print_counters(self):\n",
    "        \"\"\"helper method to show counting results\"\"\"\n",
    "        \n",
    "        now = datetime.datetime.utcnow()\n",
    "        print(\"##### Last run at {}\".format(now))\n",
    "        for timestamp, ip_counts in self.time_buckets.items():\n",
    "            # sort counts with respect to the IP address\n",
    "            ip_counts = sorted(ip_counts.items(), key=itemgetter(0))\n",
    "            print(timestamp, ':', list(ip_counts))\n",
    "            \n",
    "    def process_records(self, records):\n",
    "        for ip_addr, timestamp_str in self.iter_records(records):\n",
    "            timestamp = parser.parse(timestamp_str)\n",
    "            timestamp = timestamp.replace(second=0, microsecond=0)\n",
    "            self.time_buckets[timestamp][ip_addr] += 1\n",
    "        self.print_counters()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Last run at 2018-07-29 22:36:16.020524\n",
      "2018-07-29 22:35:00 : [('8.8.8.8', 2)]\n",
      "2018-07-29 22:36:00 : [('8.8.8.8', 7), ('8.8.8.9', 4)]\n",
      "##### Last run at 2018-07-29 22:36:36.202270\n",
      "2018-07-29 22:35:00 : [('8.8.8.8', 2)]\n",
      "2018-07-29 22:36:00 : [('8.8.8.8', 16), ('8.8.8.9', 7)]\n",
      "##### Last run at 2018-07-29 22:36:56.384823\n",
      "2018-07-29 22:35:00 : [('8.8.8.8', 2)]\n",
      "2018-07-29 22:36:00 : [('8.8.8.8', 26), ('8.8.8.9', 11)]\n",
      "##### Last run at 2018-07-29 22:37:16.581523\n",
      "2018-07-29 22:35:00 : [('8.8.8.8', 2)]\n",
      "2018-07-29 22:36:00 : [('8.8.8.8', 27), ('8.8.8.9', 12)]\n",
      "2018-07-29 22:37:00 : [('8.8.8.8', 8), ('8.8.8.9', 3)]\n",
      "##### Last run at 2018-07-29 22:37:36.753725\n",
      "2018-07-29 22:35:00 : [('8.8.8.8', 2)]\n",
      "2018-07-29 22:36:00 : [('8.8.8.8', 27), ('8.8.8.9', 12)]\n",
      "2018-07-29 22:37:00 : [('8.8.8.8', 18), ('8.8.8.9', 7)]\n"
     ]
    }
   ],
   "source": [
    "worker = CounterWorker(stream_name, shard_id, iterator_type, worker_time=120)\n",
    "worker.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream blogpost-word-stream not found. Exiting\n",
      "stream blogpost-word-stream not found. Exiting\n"
     ]
    }
   ],
   "source": [
    "# delete the stream at the end of the exercise to minimize AWS costs\n",
    "kinesis.delete_stream(stream_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sqreen-backend)",
   "language": "python",
   "name": "sqreen-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
